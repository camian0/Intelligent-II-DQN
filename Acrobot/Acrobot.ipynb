{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class DQNNetwork(nn.Module):\n",
    "    \"\"\"Red neuronal para el agente DQN.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env_name=\"Acrobot-v1\", episodes=1000, gamma=0.99, lr=0.001):\n",
    "        self.env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "        self.episodes = episodes\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.memory = []\n",
    "        self.batch_size = 64\n",
    "        self.replay_buffer_capacity = 10000\n",
    "\n",
    "        # Para las gráficas\n",
    "        self.rewards_per_episode = []\n",
    "        self.best_reward = -np.inf\n",
    "        self.losses = []  # Para las pérdidas\n",
    "        self.epsilon_values = []  # Para la decadencia de epsilon\n",
    "        self.best_rewards_per_episode = []  # Para las mejores recompensas\n",
    "\n",
    "        # Dimensiones\n",
    "        input_dim = self.env.observation_space.shape[0]\n",
    "        output_dim = self.env.action_space.n\n",
    "\n",
    "        # Inicializar la red y optimizador\n",
    "        self.network = DQNNetwork(input_dim, output_dim)\n",
    "        self.target_network = DQNNetwork(input_dim, output_dim)\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.target_network.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=self.lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Almacenar experiencias en la memoria de replay.\"\"\"\n",
    "        if len(self.memory) > self.replay_buffer_capacity:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample_memory(self):\n",
    "        \"\"\"Obtener un batch aleatorio de la memoria.\"\"\"\n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Seleccionar acción utilizando la política epsilon-greedy.\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(self.network(state)).item()\n",
    "\n",
    "    def update_network(self):\n",
    "        \"\"\"Actualizar la red neuronal usando experiencias almacenadas.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.sample_memory()\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        q_values = self.network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.target_network(next_states).max(1)[0]\n",
    "        expected_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        loss = self.loss_fn(q_values, expected_q_values.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Registrar la pérdida\n",
    "        self.losses.append(loss.item())\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(1, self.episodes + 1):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                self.store_transition(state, action, reward, next_state, done)\n",
    "                self.update_network()\n",
    "\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            self.rewards_per_episode.append(total_reward)\n",
    "            self.best_rewards_per_episode.append(self.best_reward)\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "            self.epsilon_values.append(self.epsilon)\n",
    "\n",
    "            # Actualizar la red objetivo periódicamente\n",
    "            if episode % 20 == 0:\n",
    "                self.target_network.load_state_dict(self.network.state_dict())\n",
    "\n",
    "            # Guardar el modelo si es la mejor recompensa\n",
    "            if total_reward > self.best_reward:\n",
    "                self.best_reward = total_reward\n",
    "                self.save_model()\n",
    "\n",
    "            # Imprimir métricas cada 100 episodios\n",
    "            if episode % 100 == 0:\n",
    "                mean_reward = np.mean(self.rewards_per_episode[-100:])\n",
    "                print(\n",
    "                    f\"Episode: {episode}, Epsilon: {self.epsilon:.2f}, \"\n",
    "                    f\"Best Reward: {self.best_reward:.1f}, Mean Reward: {mean_reward:.1f}\"\n",
    "                )\n",
    "\n",
    "        self.plot_rewards()\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"Guardar el modelo entrenado.\"\"\"\n",
    "        torch.save(self.network.state_dict(), \"acrobot_dqn_model.pth\")\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Cargar el modelo entrenado.\"\"\"\n",
    "        self.network.load_state_dict(torch.load(\"acrobot_dqn_model.pth\"))\n",
    "        self.network.eval()\n",
    "\n",
    "    def plot_rewards(self):\n",
    "        \"\"\"Graficar las recompensas acumuladas y métricas del entrenamiento.\"\"\"\n",
    "        plt.figure(figsize=(14, 10))\n",
    "\n",
    "        # Graficar recompensas por episodio\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(self.rewards_per_episode, label=\"Rewards per Episode\")\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Rewards\")\n",
    "        plt.title(\"DQN Training Performance on Acrobot\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Graficar la pérdida\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(self.losses, label=\"Loss per Update\", color=\"red\")\n",
    "        plt.xlabel(\"Training Steps\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss During Training\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Graficar la Epsilon Decay\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(self.epsilon_values, label=\"Epsilon Decay\", color=\"green\")\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Epsilon\")\n",
    "        plt.title(\"Epsilon Decay During Training\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Graficar la mejor recompensa suavizada\n",
    "        plt.subplot(2, 2, 4)\n",
    "        # Promedio móvil para suavizar la curva de mejores recompensas\n",
    "        window_size = 50  # Tamaño de la ventana para el promedio móvil\n",
    "        best_rewards_smooth = np.convolve(\n",
    "            self.best_rewards_per_episode,\n",
    "            np.ones(window_size) / window_size,\n",
    "            mode=\"valid\",\n",
    "        )\n",
    "\n",
    "        plt.plot(best_rewards_smooth, label=\"Smoothed Best Reward\", color=\"orange\")\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Smoothed Best Reward\")\n",
    "        plt.title(\"Smoothed Best Reward Over Time\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = DQNAgent(episodes=1000)\n",
    "    agent.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class DQNNetworkTensor(keras.Model):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQNNetworkTensor, self).__init__()\n",
    "        self.fc1 = layers.Dense(128, activation=\"relu\", input_shape=(input_dim,))\n",
    "        self.fc2 = layers.Dense(128, activation=\"relu\")\n",
    "        self.fc3 = layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.fc2(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class DQNAgentTensor:\n",
    "    def __init__(self, env_name=\"Acrobot-v1\", episodes=1000, gamma=0.99, lr=0.001):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.episodes = episodes\n",
    "        self.gamma = gamma  # Factor de descuento\n",
    "        self.lr = lr  # Tasa de aprendizaje\n",
    "        self.epsilon = 0.99  # Tasa de exploración inicial\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.memory = []  # Memoria de replay\n",
    "        self.batch_size = 64\n",
    "        self.replay_buffer_capacity = 10000\n",
    "\n",
    "        # Métricas para las gráficas\n",
    "        self.rewards_per_episode = []\n",
    "        self.losses = []  # Para almacenar las pérdidas\n",
    "        self.epsilon_values = []  # Para almacenar los valores de epsilon\n",
    "        self.best_reward = -np.inf\n",
    "        self.best_rewards_per_episode = []  # Para las mejores recompensas\n",
    "\n",
    "        # Dimensiones de entrada y salida\n",
    "        self.input_dim = self.env.observation_space.shape[0]\n",
    "        self.output_dim = self.env.action_space.n\n",
    "\n",
    "        # Inicializar la red y el optimizador\n",
    "        self.network = DQNNetworkTensor(self.input_dim, self.output_dim)\n",
    "        self.target_network = DQNNetworkTensor(self.input_dim, self.output_dim)\n",
    "        self.update_target_network()\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=self.lr)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Actualiza los pesos de la red objetivo.\"\"\"\n",
    "        self.target_network.set_weights(self.network.get_weights())\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Almacenar experiencias en la memoria de replay.\"\"\"\n",
    "        if len(self.memory) > self.replay_buffer_capacity:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample_memory(self):\n",
    "        \"\"\"Obtener un batch aleatorio de la memoria.\"\"\"\n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Seleccionar acción utilizando la política epsilon-greedy.\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        state = np.array(state).reshape(-1, self.input_dim)\n",
    "        q_values = self.network(state, training=False)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def update_network(self):\n",
    "        \"\"\"Actualizar la red neuronal usando experiencias almacenadas.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.sample_memory()\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards)\n",
    "        dones = np.array(dones).astype(float)\n",
    "\n",
    "        # Predecir Q(s,a) y Q(s',a') usando la red actual y la red objetivo\n",
    "        q_values = self.network(states, training=False)  # Predicción con la red actual\n",
    "        next_q_values = self.target_network(\n",
    "            next_states, training=False\n",
    "        )  # Predicción con la red objetivo\n",
    "\n",
    "        # Crear un tensor con los targets\n",
    "        updated_q_values = (\n",
    "            q_values.numpy().copy()\n",
    "        )  # Convertir q_values a un array de NumPy para modificarlo\n",
    "\n",
    "        # Actualizar los Q-values para las acciones tomadas\n",
    "        for i in range(self.batch_size):\n",
    "            target = rewards[i]\n",
    "            if not dones[i]:\n",
    "                target += self.gamma * np.max(\n",
    "                    next_q_values[i]\n",
    "                )  # Valor de target con descuento\n",
    "            updated_q_values[i][actions[i]] = (\n",
    "                target  # Actualizar el Q-value en el índice correspondiente\n",
    "            )\n",
    "\n",
    "        # Convertir de nuevo a un tensor de TensorFlow para el entrenamiento\n",
    "        updated_q_values_tensor = tf.convert_to_tensor(\n",
    "            updated_q_values, dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        # Entrenar la red con los estados y Q-values actualizados\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values_pred = self.network(\n",
    "                states, training=True\n",
    "            )  # Predicción de la red para entrenamiento\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.square(updated_q_values_tensor - q_values_pred)\n",
    "            )  # Cálculo de la pérdida\n",
    "\n",
    "        grads = tape.gradient(loss, self.network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.network.trainable_variables))\n",
    "\n",
    "        # Almacenar la pérdida\n",
    "        self.losses.append(loss.numpy())\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(1, self.episodes + 1):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                self.store_transition(state, action, reward, next_state, done)\n",
    "                self.update_network()\n",
    "\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "\n",
    "            self.rewards_per_episode.append(total_reward)\n",
    "            self.best_rewards_per_episode.append(self.best_reward)\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "            self.epsilon_values.append(self.epsilon)\n",
    "\n",
    "            # Actualizar la red objetivo periódicamente\n",
    "            if episode % 2 == 0:\n",
    "                self.update_target_network()\n",
    "\n",
    "            # Guardar el modelo si es la mejor recompensa\n",
    "            if total_reward > self.best_reward:\n",
    "                self.best_reward = total_reward\n",
    "                self.save_model()\n",
    "\n",
    "            # Mostrar estadísticas cada 100 episodios\n",
    "            mean_reward = np.mean(self.rewards_per_episode[-100:])\n",
    "            print(\n",
    "                f\"Episodio: {episode}, Epsilon: {self.epsilon:.2f}, \"\n",
    "                f\"Mejor Recompensa: {self.best_reward:.1f}, Recompensa Promedio: {mean_reward:.1f}\"\n",
    "            )\n",
    "\n",
    "        self.plot_metrics()\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"Guardar el modelo entrenado.\"\"\"\n",
    "        self.network.save(\"acrobot_dqn_model.keras\")\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Cargar el modelo entrenado.\"\"\"\n",
    "        self.network = keras.models.load_model(\"acrobot_dqn_model.keras\")\n",
    "        self.update_target_network()\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        \"\"\"Graficar las métricas de rendimiento durante el entrenamiento.\"\"\"\n",
    "        plt.figure(figsize=(14, 10))\n",
    "\n",
    "        # Graficar recompensas por episodio\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(self.rewards_per_episode, label=\"Recompensa por Episodio\")\n",
    "        plt.xlabel(\"Episodios\")\n",
    "        plt.ylabel(\"Recompensa\")\n",
    "        plt.title(\"Recompensas por Episodio\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Graficar la pérdida\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(self.losses, label=\"Pérdida por Actualización\", color=\"red\")\n",
    "        plt.xlabel(\"Pasos de Entrenamiento\")\n",
    "        plt.ylabel(\"Pérdida\")\n",
    "        plt.title(\"Pérdida Durante el Entrenamiento\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Graficar la Epsilon Decay\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(self.epsilon_values, label=\"Decadencia de Epsilon\", color=\"green\")\n",
    "        plt.xlabel(\"Episodios\")\n",
    "        plt.ylabel(\"Epsilon\")\n",
    "        plt.title(\"Decadencia de Epsilon Durante el Entrenamiento\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Graficar la mejor recompensa suavizada\n",
    "        plt.subplot(2, 2, 4)\n",
    "        # window_size = 50  # Tamaño de la ventana para el promedio móvil\n",
    "        # best_rewards_smooth = np.convolve(\n",
    "        #     self.best_rewards_per_episode,\n",
    "        #     np.ones(window_size) / window_size,\n",
    "        #     mode=\"valid\",\n",
    "        # )\n",
    "\n",
    "        plt.plot(\n",
    "            self.best_rewards_per_episode,\n",
    "            label=\"Recompensa Mejorada Suavizada\",\n",
    "            color=\"orange\",\n",
    "        )\n",
    "        plt.xlabel(\"Episodios\")\n",
    "        plt.ylabel(\"Recompensa Mejorada Suavizada\")\n",
    "        plt.title(\"Mejor Recompensa Suavizada\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = DQNAgentTensor(episodes=10)\n",
    "    agent.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
