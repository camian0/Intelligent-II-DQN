{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacion del modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(state_dim, action_dim):\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Dense(128, input_dim=state_dim, activation=\"relu\"),\n",
    "            Dense(128, activation=\"relu\"),\n",
    "            Dense(action_dim, activation=\"linear\"),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase DQNAgent con historial de pérdidas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        replay_size=10000,\n",
    "        batch_size=64,\n",
    "        gamma=0.99,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        epsilon_min=0.01,\n",
    "        discretization_bins=10,  # Nuevas variables para la discretización\n",
    "    ):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.replay_buffer = deque(maxlen=replay_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.loss_history = []  # Para almacenar la pérdida promedio por episodio\n",
    "        self.mae_history = []  # Para almacenar el MAE promedio por episodio\n",
    "        self.discretization_bins = (\n",
    "            discretization_bins  # Número de intervalos para discretizar cada dimensión\n",
    "        )\n",
    "\n",
    "        # Redes Q y Target\n",
    "        self.q_network = build_model(state_dim, action_dim)\n",
    "        self.target_network = build_model(state_dim, action_dim)\n",
    "        self.update_target_network()\n",
    "\n",
    "        self.bins = [\n",
    "            np.linspace(-1, 1, discretization_bins),  # Cos(theta1)\n",
    "            np.linspace(-1, 1, discretization_bins),  # Sin(theta1)\n",
    "            np.linspace(-1, 1, discretization_bins),  # Cos(theta2)\n",
    "            np.linspace(-1, 1, discretization_bins),  # Sin(theta2)\n",
    "            np.linspace(-12.567, 12.567, discretization_bins),  # Velocidad theta1\n",
    "            np.linspace(-28.274, 28.274, discretization_bins),  # Velocidad theta2\n",
    "        ]\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Selecciona una acción usando ε-greedy.\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        q_values = self.q_network.predict(state, verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, terminated):\n",
    "        \"\"\"Guarda la experiencia discretizando los estados.\"\"\"\n",
    "        discretized_state = self.discretize_state(state)\n",
    "        discretized_next_state = self.discretize_state(next_state)\n",
    "        self.replay_buffer.append(\n",
    "            (discretized_state, action, reward, discretized_next_state, terminated)\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Entrena la red con experiencias almacenadas.\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, terminateds = zip(*batch)\n",
    "\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "\n",
    "        q_values = self.q_network.predict(states, verbose=0)\n",
    "        q_values_next = self.target_network.predict(next_states, verbose=0)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            if terminateds[i]:\n",
    "                q_values[i, actions[i]] = rewards[i]\n",
    "            else:\n",
    "                q_values[i, actions[i]] = rewards[i] + self.gamma * np.max(\n",
    "                    q_values_next[i]\n",
    "                )\n",
    "\n",
    "        history = self.q_network.fit(states, q_values, epochs=1, verbose=0)\n",
    "        self.loss_history.append(history.history[\"loss\"][0])\n",
    "        self.mae_history.append(history.history[\"mae\"][0])\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        \"\"\"Convierte un estado continuo a uno discreto.\"\"\"\n",
    "        discretized = []\n",
    "        for i, feature in enumerate(state):\n",
    "            discretized.append(np.digitize(feature, self.bins[i]) - 1)\n",
    "        return tuple(discretized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(scores, label=\"Puntaje por episodio\")\n",
    "    plt.xlabel(\"Episodio\")\n",
    "    plt.ylabel(\"Puntaje\")\n",
    "    plt.title(\"Evolución del Puntaje\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss(loss_history):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(loss_history, label=\"Pérdida por batch\")\n",
    "    plt.xlabel(\"Batch\")\n",
    "    plt.ylabel(\"Pérdida (Loss)\")\n",
    "    plt.title(\"Evolución de la Pérdida\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_mae(mae_history):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(mae_history, label=\"MAE por batch\")\n",
    "    plt.xlabel(\"Batch\")\n",
    "    plt.ylabel(\"Error Absoluto Medio (MAE)\")\n",
    "    plt.title(\"Evolución del MAE\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del DQN con métricas y video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(\n",
    "    env_name=\"Acrobot-v1\",\n",
    "    episodes=10,\n",
    "    target_update=10,\n",
    "    video_filename=\"dqn_training_video.mp4\",\n",
    "):\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_dim, action_dim)\n",
    "    scores = []\n",
    "\n",
    "    env.reset()\n",
    "    sample_frame = env.render()[0]\n",
    "    height, width, channels = (\n",
    "        sample_frame.shape\n",
    "        if len(sample_frame.shape) == 3\n",
    "        else (sample_frame.shape[0], sample_frame.shape[1], 1)\n",
    "    )\n",
    "\n",
    "    obj_video = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(\n",
    "        video_filename, obj_video, 30, (width, height), isColor=(channels == 3)\n",
    "    )\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "        episode_score = 0\n",
    "\n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            next_state = np.array(next_state, dtype=np.float32)\n",
    "            episode_score += reward\n",
    "\n",
    "            target_height = -np.cos(next_state[0]) - np.cos(\n",
    "                next_state[2] + next_state[0]\n",
    "            )\n",
    "            reward += target_height * 0.5\n",
    "\n",
    "            if terminated and episode_score < -100:\n",
    "                reward = -10\n",
    "\n",
    "            agent.store_experience(state, action, reward, next_state, terminated)\n",
    "            agent.train()\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            frame = env.render()[0]\n",
    "            frame_bgr = (\n",
    "                cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                if len(frame.shape) == 3\n",
    "                else cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
    "            )\n",
    "            out.write(frame_bgr)\n",
    "\n",
    "            if terminated:\n",
    "                break\n",
    "\n",
    "        agent.epsilon = max(agent.epsilon * agent.epsilon_decay, agent.epsilon_min)\n",
    "        scores.append(episode_score)\n",
    "\n",
    "        if episode % target_update == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode}, Score: {episode_score}, Epsilon: {agent.epsilon:.2f}\"\n",
    "        )\n",
    "\n",
    "    out.release()\n",
    "    env.close()\n",
    "\n",
    "    plot_scores(scores)\n",
    "    plot_loss(agent.loss_history)\n",
    "    plot_mae(agent.mae_history)\n",
    "\n",
    "    return scores, agent.loss_history, agent.mae_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar y graficar resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Score: -1274.0, Epsilon: 0.99\n",
      "Episode 1, Score: -1282.0, Epsilon: 0.99\n",
      "Episode 2, Score: -1536.0, Epsilon: 0.99\n",
      "Episode 3, Score: -1299.0, Epsilon: 0.98\n",
      "Episode 4, Score: -3649.0, Epsilon: 0.98\n",
      "Episode 5, Score: -919.0, Epsilon: 0.97\n",
      "Episode 6, Score: -6288.0, Epsilon: 0.97\n"
     ]
    }
   ],
   "source": [
    "scores, loss_history, mae_history = train_dqn(episodes=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
